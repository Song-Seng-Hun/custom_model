import sys, os
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))
from gemma.tokenizer import Tokenizer
import sentencepiece as spm
tokenizer = Tokenizer('/home/medi/LLM/gemma_pytorch/tokenizer/tokenizer.model')
lines = [
  "`DEVOCEAN`ì€ SKê·¸ë£¹ì˜ ëŒ€í‘œ ê°œë°œì ì»¤ë®¤ë‹ˆí‹°ì´ìğŸ§‘",
  "ë‚´/ì™¸ë¶€ ê°œë°œì ê°„ ì†Œí†µê³¼ ì„±ì¥ì„ ìœ„í•œ í”Œë«í¼ì„ ìƒì§•í•©ë‹ˆë‹¤.ğŸ‘‹",
  "`Developers`' Ocean ê°œë°œìë“¤ì„ ìœ„í•œ ì˜ê°ì˜ ë°”ë‹¤ğŸ™",
  "`Devotion` í—Œì‹ ,ëª°ë‘,ì „ë…ğŸ’¯",
  "`Technology for Everyone` ëª¨ë‘ë¥¼ ìœ„í•œ ê¸°ìˆ ğŸ‘"
  ]

for line in lines:
    inputs = tokenizer.encode(line)    
    print(inputs)  
    decoded_sequence = tokenizer.decode(inputs[0])
    print(decoded_sequence)
    print()
    print(tokenizer.decode(7))
